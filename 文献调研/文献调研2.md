# 文献调研2

大语言模型主要分为两大类框架：生成式新闻推荐框架+判别式新闻推荐框架
1.  生成式新闻推荐，Recprompt，新闻推荐器大模型，接受用户历史和候选新闻，用于生成候选新闻的排序列表和解释性文本；提示词优化大模型结果反馈优化大模型。GNR，利用大模型将筛选出的多篇新闻融合为一篇连贯且个性化的叙述文本。
2.  判别式新闻推荐，ONCE，开源模型，迁移网络参数对新闻和用户建模，闭源大模型，用提示词构建辅助训练数据。PNR-LLM，用大模型对新闻文本进行重写和实体抽取，可以移植到各个框架模型当中；SINGLE，长期偏好——提取用户的职位、技能，即时浏览兴趣建模。
3.  智能体在推荐系统中应用：推荐导向，交互导向，仿真导向。

# Streaming In-context Learning for Dynamic Interest Adaption in LLM-based Recommendation

## 文章来源
[RecICL.2025.findings-acl.735.pdf](RecICL.2025.findings-acl.735.pdf)
## 主要工作
### 作者认为的主要问题
动态环境中瞬时兴趣的变化，用户兴趣可能会迅速转变。通常使用传入的数据定期更新模型，以捕捉及时的用户兴趣。然而，由于LLM中存在大量的参数，这种模型级别的更新会导致基于LLM的推荐产生巨大的计算和时间成本，使其在实际应用中不切实际。ICL模型上下文学习就是一种对模型本身参数无需更新的一种方式
![[Pasted image 20260118171732.png]]
![[Pasted image 20260118171840.png]]
![[Pasted image 20260118172124.png]]
![[Pasted image 20260118172141.png]]

## dataset
Amazon-Movies & Amazon-Books

# AGRec: Adapting Autoregressive Decoders with Graph Reasoning for LLM-based Sequential Recommendation
## 文章来源
[AGRec.2025.findings-acl.369.pdf](AGRec.2025.findings-acl.369.pdf)
## 摘要
(1) 我们揭示了大型语言模型（LLMs）和图神经网络（GNNs）在推荐任务中的互补优势，并提出了AGRec，它利用辅助GNN来增强LLMs的token生成；
(2) 我们引入了一种新颖的可排序有限状态机（FSM）来解决LLMs和GNNs之间的解码错位问题以及同质性问题。
(3) 大量实验表明，以LLaMA-1B为骨干的AGRec在序列推荐任务中优于配备LLaMA7B的最先进（SOTA）基线。
## 存在的问题
1. LLM 的一个固有问题是它们难以解释图结构的高阶交互，而这些交互被广泛认为对推荐任务有价值
2. LLM的自回归解码器涉及从预定义词汇表中选择token，其核心在于文本连贯性，这与GNN依赖用户-项目表示相似性的解码方式存在本质区别。
![[Pasted image 20260118180903.png]]

3. 用户-物品交互的极端稀疏性（如表1所示，小于0.09%），训练具有显著大词汇量的推荐大语言模型（其中单个token代表每个物品）非常具有挑战性。
![[Pasted image 20260118181011.png]]
4. 语言空间中物品的非均匀分布进一步加剧了这种情况，导致某些物品包含生成概率接近1的token。此外，一旦前缀token不正确，无论后续的token如何，模型都无法做出推荐。相比之下，由GNN解码的物品token提供了更大的多样性，使大语言模型能够拓宽其在选择前缀token时的视角。

## 主要工作
1. 综合了LLM和GNN，也就是用lightGCN辅助LLMtoken的生成
![[Pasted image 20260119222636.png]]
2. FSM状态机机制来回溯不同节点并同时计算相似度，保证不会非法乱跳
![[Pasted image 20260119222950.png]]
3. item_tokennization,用树结构替代直接的文本id的编码，更好的能反映出语义

## 个人想法
没太看懂这个状态机的作用
主要可能还是llm和gnn结合这个想法不错，但是加权结合还是牵强了

# Generative News Recommendation

## 文献来源

## 摘要
现有的大多数新闻推荐方法通过对候选新闻和由历史点击新闻产生的用户表示进行语义匹配来解决此任务。然而，它们忽略了不同新闻文章之间的高层次联系，也忽略了这些新闻文章与用户之间的深刻关系。并且这些方法的定义决定了它们只能按原样传递新闻文章。相反，将几篇相关的新闻文章整合到一个连贯的叙述中，将有助于用户更快、更全面地了解事件。在本文中，我们提出了一种新的生成式新闻推荐范式，包括两个步骤：（1）利用大型语言模型（LLM）的内部知识和推理能力，对候选新闻和用户表示进行高层次匹配；（2）基于相关新闻和用户兴趣之间的关联，生成连贯且逻辑结构化的叙述，从而吸引用户进一步阅读新闻。具体来说，我们提出了GNR来实现生成式新闻推荐范式。首先，我们利用LLM生成主题层面的表示，并将其与语义层面的表示相结合，从而构成新闻和用户的双层表示。接下来，为了生成连贯的叙述，我们探索新闻关系，并根据用户偏好过滤相关新闻。最后，我们提出了一种名为UIFT的新型训练方法，以训练LLM将多篇新闻文章融合到一个连贯的叙述中。大量的实验表明，GNR可以提高推荐准确性，并最终生成更个性化和事实一致的叙述。

## 提出的主要问题
1. 还是隐因子的问题，不同新闻之间存在的更深层次的联系，表面看起来不相干，实际上基于某种共享的主题和一些默认的事实，可以联系在一起
2. 原封不动照搬文章非常冗余，不如合成一段话。
## 主要工作
1. 三个模块：用户画像+新闻的主题级别表示————检索并筛选感兴趣的信息————整合成一段话
2. 获取主题级别的表示并给出参考新闻集：
![[Pasted image 20260119230815.png]]
用户和新闻的双层表示：类似于LightGCN
选择最高的那条新闻作为焦点新闻
对比学习得到相关新闻集
![[Pasted image 20260120095120.png]]
进一步过滤：相关新闻集和用户嵌入之间的匹配得分，得到T-1篇加上最高的那个焦点新闻，构成T条新闻构成的参考新闻集
3. 整合
UIFT微调
![[Pasted image 20260120101937.png]]
![[Pasted image 20260120102123.png]]
dataset：MIND Adressa
## 个人想法
1. 比较严谨，但是本质上还是只解释了怎么合成一段话。高阶关系的获取，可解释性不强
2. 冷启动问题

# CROWN: A Novel Approach to Comprehending Users' Preferences for Accurate Personalized News Recommendation

## 提出的主要问题
1. 一篇新闻有多种意图
```
一篇报道：表面上宣传新政策，实质上有挑动民众的反对情绪，再其次有想法去收听民众意见
```
2. 点击$\neq$对新闻感兴趣。有可能只是刷到了但是点开浏览了一下就退出了
3. 冷启动问题
## 主要工作
![[Pasted image 20260120185425.png]]
- **新闻编码器**：采用类别引导的意图解缠（使用多头注意力块和类别嵌入生成多个意图嵌入），以及基于标题-内容一致性的新闻表示（通过余弦相似度计算一致性分数，并据此加权聚合标题和内容嵌入）。
- **用户编码器**：构建用户-新闻二部图，并使用GNN（具体为GAT）进行节点嵌入更新，以整合内容信号和协作信号，实现混合用户表示；随后通过用户特定注意力机制聚合点击新闻嵌入。
- **预测模块**：点击预测器（主任务）使用用户嵌入和候选新闻标题嵌入的内积计算点击概率；类别预测器（辅助任务）使用新闻嵌入预测类别，提供额外监督信号。
- **训练与优化**：端到端联合优化点击预测和类别预测损失，使用负采样和Adam优化器；实验基于MIND和Adressa数据集，评估指标包括AUC、MRR、nDCG@5/10。
## 个人思考
1. 文章看起来很舒服，清晰易懂。这点是前面两篇文章比不了的，解释的很清晰，没有什么弯弯绕。
2. 但是问题在于：所谓的“意图”“是否感兴趣”，被形象化为一个具体的神经网络中的可学习的参数而已，并不是真正实现了网络可以识别意图了。这一方面是否可以再加强？
3. 这篇文章不涉及LLM。但是我觉得框架很有创新性
4. 冷启动问题的解决：用图神经网络协同过滤。这个是多少年前就用的东西了……

# Benchmarking News Recommendation in the Era of Green AI
本质上是提出一个关于生态环境保护的评估标准框架，并且进行了评价

# Legommenders: A Comprehensive Content-Based Recommendation Library with LLM Support
## 主要问题
1. 冷启动问题
2. 内容算子大多与后面的用户嵌入，预测器解耦，也就是说内容算子自己编码自己的，和后面的事情无关。导致了后续初始的嵌入过于通用化了，上下文对不齐。
## 主要工作
![[Pasted image 20260120185328.png]]
1. 把内容嵌入和后续的预测形成一个整体网络进行训练
2. 像lego积木一样，每个模块自己选择拼装：数据处理模块，处理成统一形式，内容算子模块可以用CNN/Attention，大模型也可以用不同的huggingface上面的模型，行为预测器，用各种机制：average pool，GRU……
3. 内容嵌入和用户嵌入先缓存，然后一块儿运行点击预测，提升效率（类似于vllm？并行推理模块）
## 个人想法
挺好，非常写的简洁易懂。这种搭积木的想法也不错。冷启动的解决方案是随机初始化
lora微调可以自己选择层数，可否改为自己选择位置？

# Enhancing News Recommendation with Hierarchical LLM Prompting

## 解决问题
1. 注意力机制，长短期记忆，没有考虑文章之间的结构化关系
2. 稀疏性问题，语义浅薄，用知识图谱表示不了这种稀疏性
3. ONCE，用LLM进行动态和语义丰富的推荐
## 主要工作
将LLM用于缓解数据稀疏性，辅助性的新闻推荐，而不是生成摘要
![[Pasted image 20260120191926.png]]
1. News Enrichment
![[Pasted image 20260120192207.png]]
2. News Encoder
![[Pasted image 20260120192255.png]]
## 个人想法
本质上还是一个非常简洁的论文，各个部分内容易于理解。
还是主要谈谈为什么可以解决提到的问题：
1. 稀疏性问题：标题太短，所以进行了扩写，深度挖掘标题中的实体之间的深刻信息或者联系。
但是问题在于，有时候标题不一定就会包含这些信息呀，比如：震惊！这群人就这样一生被改变了我们提取不出来什么呀
2. 让实体之间用注意力机制学习联系。“Ukraine”和“Trump”，在传统的词向量空间里，它们的相似度可能很低；但 PNR-LLM 提取出的“实体列表”会强制让模型识别到这两者之间的强关联。

# Harnessing Large Language Models for Text-Rich Sequential Recommendation

## 主要问题
历史序列过长，LLM有输入长度限制
transformer的平方级计算量增长

## 主要工作
![[Pasted image 20260120194328.png]]
Summerizer部分分为CNN和RNN两种
![[Pasted image 20260120212403.png]]
![[Pasted image 20260120212421.png]]
![[Pasted image 20260120212451.png]]


## 个人想法
1. RNN纯没用啊，做到了弊端很明显，优势不明显
2. 对算力要求肯定很大，从示例就可以看出输入文本很多。
3. 非常纯粹的使用大模型干完全部过程，个人感觉没必要。并且速度是否够快也是个问题

# ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models

## 那篇经典文章 you only look once
直接改变了目标检测届。那么，是否有机会在文本领域也能实现once呢？
![[Pasted image 20260120221306.png]]
## 问题所在
1. 隐私问题，把东西都提供给了GPT这种开源模型
2. api调用的时延

# RecPrompt: A Self-tuning Prompting Framework for News Recommendation Using Large Language Models

## 主要解决的问题
1. 说是性能上还是不如Deep Neural Method
2. 需要人工评估
3. 没有给生成推荐的解释进行评估。也就是为什么会给出这些推荐呢？

## 主要工作
![[Pasted image 20260121140112.png]]
1. 解释的评估指标：TopicScore
2. ![[Pasted image 20260121140826.png]]
3. 监视器，确定是否可以用现在这个提示词模板
4. 提示词调优

## 个人想法
1. 好处就是整个过程不需要人工微调
2. 用的Openai的API，响应速度和隐私都受影响。换成本地大模型
3. 人工评分还是不行呀，得要全自动

# LLM-based Conversational Recommendation Agents with Collaborative Verbalized Experience

2025emnlp
## 主要问题
针对对话推荐系统，冷启动问题

## 主要工作
![[Pasted image 20260121150343.png]]
1. 检索网络用于微调
2. 几个agent主要是不断迭代，类似于重排序？生成经验之谈，便于后续的检索

# A Study of Implicit Ranking Unfairness in Large Language Models

## 主要问题
1. 

## 主要工作
